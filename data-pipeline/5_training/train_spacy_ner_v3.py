"""
Training Script NER v3.0 - VERSION FINALE
==========================================
Entra√Æne un mod√®le spaCy NER pour d√©tecter :
- ‚úÖ INGREDIENT (classiques + E-numbers + min√©raux + vitamines)
- ‚úÖ QUANTITY (%, g, mg, ml, etc.)
- ‚úÖ ALLERGEN (14 cat√©gories EU)

Auteur: EcoLabel-MS Data Pipeline
Date: 2025
"""

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import json
import random
from pathlib import Path
import sys
from datetime import datetime

sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log
from utils.file_utils import ensure_dir


def load_annotations(jsonl_file: str):
    """Charge les annotations depuis un fichier JSONL"""
    training_data = []
    
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            training_data.append((
                data['text'],
                {'entities': data['entities']}
            ))
    
    log.info(f"‚úÖ Charg√©: {len(training_data)} exemples depuis {jsonl_file}")
    return training_data


def split_data(data, train_ratio=0.7, val_ratio=0.15):
    """
    Divise les donn√©es en train/validation/test.
    
    Args:
        data: Liste de tuples (text, annotations)
        train_ratio: Ratio d'entra√Ænement (0.7 = 70%)
        val_ratio: Ratio de validation (0.15 = 15%)
    
    Returns:
        train_data, val_data, test_data
    """
    random.shuffle(data)
    
    n = len(data)
    train_end = int(n * train_ratio)
    val_end = train_end + int(n * val_ratio)
    
    train_data = data[:train_end]
    val_data = data[train_end:val_end]
    test_data = data[val_end:]
    
    log.info(f"üìä Split des donn√©es:")
    log.info(f"   ‚Ä¢ Train:      {len(train_data):4d} ({len(train_data)/n*100:.1f}%)")
    log.info(f"   ‚Ä¢ Validation: {len(val_data):4d} ({len(val_data)/n*100:.1f}%)")
    log.info(f"   ‚Ä¢ Test:       {len(test_data):4d} ({len(test_data)/n*100:.1f}%)")
    
    return train_data, val_data, test_data


def save_split_data(train_data, val_data, test_data, output_dir):
    """Sauvegarde les splits dans des fichiers JSONL s√©par√©s"""
    ensure_dir(output_dir)
    
    splits = {
        'train': train_data,
        'validation': val_data,
        'test': test_data
    }
    
    for split_name, data in splits.items():
        output_file = Path(output_dir) / f"{split_name}.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for text, annotations in data:
                item = {
                    'text': text,
                    'entities': annotations['entities']
                }
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        log.info(f"‚úÖ Sauvegard√©: {output_file} ({len(data)} exemples)")


def evaluate_model(nlp, examples):
    """√âvalue le mod√®le sur un ensemble d'exemples"""
    if not examples:
        return {}
    
    scorer = nlp.evaluate(examples)
    return scorer


def train_ner_model(
    train_data,
    val_data,
    output_dir,
    n_iter=50,
    batch_size=16,
    drop=0.2
):
    """
    Entra√Æne un mod√®le NER spaCy.
    
    Args:
        train_data: Donn√©es d'entra√Ænement
        val_data: Donn√©es de validation
        output_dir: R√©pertoire de sortie du mod√®le
        n_iter: Nombre d'it√©rations
        batch_size: Taille des batches
        drop: Taux de dropout
    
    Returns:
        nlp: Mod√®le entra√Æn√©
    """
    log.info("=" * 80)
    log.info("üöÄ ENTRA√éNEMENT DU MOD√àLE NER v3.0 FINAL")
    log.info("=" * 80)
    log.info(f"   ‚Ä¢ It√©rations: {n_iter}")
    log.info(f"   ‚Ä¢ Batch size: {batch_size}")
    log.info(f"   ‚Ä¢ Dropout:    {drop}")
    log.info(f"   ‚Ä¢ Output:     {output_dir}")
    log.info("")
    
    # Charger le mod√®le de base fran√ßais
    log.info("üì¶ Chargement du mod√®le de base 'fr_core_news_md'...")
    nlp = spacy.load("fr_core_news_md")
    
    # Cr√©er le pipeline NER s'il n'existe pas
    if "ner" not in nlp.pipe_names:
        ner = nlp.add_pipe("ner", last=True)
    else:
        ner = nlp.get_pipe("ner")
    
    # Ajouter les labels
    log.info("üè∑Ô∏è  Ajout des labels NER...")
    labels = set()
    for _, annotations in train_data:
        for ent in annotations.get("entities"):
            labels.add(ent[2])
    
    for label in labels:
        ner.add_label(label)
    
    log.info(f"‚úÖ Labels: {sorted(labels)}")
    
    # Pr√©parer les exemples d'entra√Ænement
    log.info("\nüìù Pr√©paration des exemples...")
    train_examples = []
    for text, annotations in train_data:
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, annotations)
        train_examples.append(example)
    
    # Pr√©parer les exemples de validation
    val_examples = []
    if val_data:
        for text, annotations in val_data:
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annotations)
            val_examples.append(example)
    
    log.info(f"‚úÖ Train:      {len(train_examples)} exemples")
    log.info(f"‚úÖ Validation: {len(val_examples)} exemples")
    
    # D√©sactiver les autres pipes pendant l'entra√Ænement
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
    
    # Entra√Ænement
    log.info(f"\nüéØ D√âBUT DE L'ENTRA√éNEMENT ({n_iter} it√©rations)...")
    log.info("")
    
    start_time = datetime.now()
    best_f1 = 0.0
    best_iteration = 0
    
    with nlp.disable_pipes(*other_pipes):
        optimizer = nlp.resume_training()
        
        for iteration in range(n_iter):
            random.shuffle(train_examples)
            losses = {}
            
            # Batch training
            batches = minibatch(train_examples, size=compounding(4.0, batch_size, 1.001))
            for batch in batches:
                nlp.update(
                    batch,
                    drop=drop,
                    sgd=optimizer,
                    losses=losses
                )
            
            # √âvaluation sur validation tous les 5 it√©rations
            if (iteration + 1) % 5 == 0 or iteration == 0:
                if val_examples:
                    scores = evaluate_model(nlp, val_examples)
                    f1 = scores.get("ents_f", 0.0) * 100
                    precision = scores.get("ents_p", 0.0) * 100
                    recall = scores.get("ents_r", 0.0) * 100
                    
                    # Suivre le meilleur F1
                    if f1 > best_f1:
                        best_f1 = f1
                        best_iteration = iteration + 1
                    
                    log.info(
                        f"Iter {iteration + 1:3d} | "
                        f"Loss: {losses.get('ner', 0.0):8.2f} | "
                        f"P: {precision:5.2f}% | "
                        f"R: {recall:5.2f}% | "
                        f"F1: {f1:5.2f}% "
                        f"{'üî•' if f1 == best_f1 else ''}"
                    )
                else:
                    log.info(
                        f"Iter {iteration + 1:3d} | "
                        f"Loss: {losses.get('ner', 0.0):8.2f}"
                    )
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    log.info("")
    log.info("‚úÖ ENTRA√éNEMENT TERMIN√â !")
    log.info(f"   ‚Ä¢ Dur√©e:      {duration}")
    log.info(f"   ‚Ä¢ Meilleur F1: {best_f1:.2f}% (it√©ration {best_iteration})")
    
    # Sauvegarder le mod√®le
    log.info(f"\nüíæ Sauvegarde du mod√®le dans {output_dir}...")
    ensure_dir(output_dir)
    nlp.to_disk(output_dir)
    log.info("‚úÖ Mod√®le sauvegard√© !")
    
    return nlp


def test_model(nlp, test_data):
    """Teste le mod√®le sur l'ensemble de test"""
    log.info("\n" + "=" * 80)
    log.info("üß™ √âVALUATION SUR L'ENSEMBLE DE TEST")
    log.info("=" * 80)
    
    # Pr√©parer les exemples de test
    test_examples = []
    for text, annotations in test_data:
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, annotations)
        test_examples.append(example)
    
    # √âvaluer
    scores = evaluate_model(nlp, test_examples)
    
    # Afficher les r√©sultats
    log.info(f"\nüìä R√âSULTATS GLOBAUX:")
    log.info(f"   ‚Ä¢ Pr√©cision: {scores.get('ents_p', 0.0) * 100:.2f}%")
    log.info(f"   ‚Ä¢ Rappel:    {scores.get('ents_r', 0.0) * 100:.2f}%")
    log.info(f"   ‚Ä¢ F1-Score:  {scores.get('ents_f', 0.0) * 100:.2f}%")
    
    # R√©sultats par entit√©
    if 'ents_per_type' in scores:
        log.info(f"\nüìä R√âSULTATS PAR TYPE D'ENTIT√â:")
        for ent_type, metrics in sorted(scores['ents_per_type'].items()):
            p = metrics.get('p', 0.0) * 100
            r = metrics.get('r', 0.0) * 100
            f = metrics.get('f', 0.0) * 100
            log.info(f"   ‚Ä¢ {ent_type:12s} ‚Üí P: {p:5.2f}%  R: {r:5.2f}%  F1: {f:5.2f}%")
    
    log.info("=" * 80)
    
    return scores


def main():
    """Fonction principale"""
    # Configuration
    ANNOTATIONS_FILE = "datasets/preprocessed/ner_annotations_v3.jsonl"
    SPLITS_DIR = "datasets/preprocessed/splits_v3"
    MODEL_OUTPUT_DIR = "models/ner_ingredients_v3"
    
    N_ITER = 50
    BATCH_SIZE = 16
    DROPOUT = 0.2
    
    # 1. Charger les annotations
    log.info("=" * 80)
    log.info("üì¶ CHARGEMENT DES DONN√âES")
    log.info("=" * 80)
    all_data = load_annotations(ANNOTATIONS_FILE)
    
    # 2. Diviser en train/val/test
    train_data, val_data, test_data = split_data(all_data)
    
    # 3. Sauvegarder les splits
    save_split_data(train_data, val_data, test_data, SPLITS_DIR)
    
    # 4. Entra√Æner le mod√®le
    nlp = train_ner_model(
        train_data=train_data,
        val_data=val_data,
        output_dir=MODEL_OUTPUT_DIR,
        n_iter=N_ITER,
        batch_size=BATCH_SIZE,
        drop=DROPOUT
    )
    
    # 5. Tester sur l'ensemble de test
    test_scores = test_model(nlp, test_data)
    
    # 6. R√©sum√© final
    log.info("\n" + "=" * 80)
    log.info("üéâ MOD√àLE NER v3.0 FINAL ENTRA√éN√â AVEC SUCC√àS !")
    log.info("=" * 80)
    log.info(f"üìÅ Mod√®le sauvegard√©: {MODEL_OUTPUT_DIR}")
    log.info(f"üéØ F1-Score final:    {test_scores.get('ents_f', 0.0) * 100:.2f}%")
    log.info("")
    log.info("üí° PROCHAINES √âTAPES:")
    log.info("   1. Copier le mod√®le dans backend/parser-service/app/models/")
    log.info("   2. Tester avec le Parser Service")
    log.info("   3. Int√©grer dans le pipeline complet")
    log.info("=" * 80)


if __name__ == "__main__":
    random.seed(42)
    main()

